# ğŸ“Š OCR Benchmark Guide

GuÃ­a completa para ejecutar y analizar benchmarks de estrategias de extracciÃ³n de texto OCR.

## ğŸ¯ Objetivo

Comparar el desempeÃ±o y confiabilidad de tres estrategias de OCR:

1. **Tesseract Local** - pypdf + pytesseract (local, gratis, bÃ¡sico)
2. **Saptiva OCR** - Actualmente en producciÃ³n (API Saptiva, robusto, probado)
3. **DeepSeek OCR** - Propuesta nueva (vÃ­a HuggingFace/AlphaXiv, alta calidad)

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno Requeridas

AÃ±ade a `envs/.env`:

```bash
# DeepSeek OCR (opcional - solo si quieres incluirlo en el benchmark)
HF_OCR_ENDPOINT=https://saptivaDev1-DeepSeek-OCR-Space.hf.space/ocr
HF_TOKEN=your_huggingface_token_here
```

**Nota**: Si no configuras DeepSeek, el benchmark solo compararÃ¡ Tesseract vs Saptiva OCR.

## ğŸš€ Uso RÃ¡pido

### OpciÃ³n 1: Comando Make (Recomendado)

```bash
# Desde la raÃ­z del proyecto
make test-benchmark
```

Esto ejecuta el benchmark con configuraciÃ³n por defecto:
- **PDF**: Capital414_presentacion.pdf
- **PÃ¡ginas**: Primeras 3 pÃ¡ginas
- **Output**: tests/reports/

### OpciÃ³n 2: Comando Manual

```bash
# Dentro del contenedor API
docker exec octavios-api python -m tests.test_ocr_benchmark \
    --pdf /app/../../tests/data/capital414/Capital414_presentacion.pdf \
    --pages 3 \
    --output /app/../../tests/reports
```

### OpciÃ³n 3: Comando Directo con Python

```bash
cd apps/api
python -m tests.test_ocr_benchmark \
    --pdf ../../tests/data/capital414/Capital414_presentacion.pdf \
    --pages 3 \
    --output ../../tests/reports \
    --deepseek-endpoint "$HF_OCR_ENDPOINT" \
    --deepseek-token "$HF_TOKEN"
```

## ğŸ“‹ Opciones de LÃ­nea de Comandos

| OpciÃ³n | DescripciÃ³n | Default | Ejemplos |
|--------|-------------|---------|----------|
| `--pdf` | Path al PDF de prueba | (requerido) | `Capital414_presentacion.pdf` |
| `--pages` | PÃ¡ginas a probar | `1,2,3` | `3`, `1,5,10`, `1-5` |
| `--output` | Directorio de reportes | `tests/reports` | `tests/benchmark_results` |
| `--deepseek-endpoint` | Endpoint de DeepSeek | (opcional) | `https://api.example.com/ocr` |
| `--deepseek-token` | Token de autenticaciÃ³n | (opcional) | `hf_xxxxx` |

### Ejemplos de Uso

```bash
# Probar solo primera pÃ¡gina (rÃ¡pido)
make test-benchmark PAGES=1

# Probar pÃ¡ginas especÃ­ficas
python -m tests.test_ocr_benchmark --pdf test.pdf --pages 1,3,5

# Probar rango de pÃ¡ginas
python -m tests.test_ocr_benchmark --pdf test.pdf --pages 1-10

# Usar otro PDF
python -m tests.test_ocr_benchmark --pdf tests/data/capital414/Capital414_usoIA.pdf --pages 5
```

## ğŸ“Š Resultados Generados

El benchmark genera automÃ¡ticamente 2 archivos en `tests/reports/`:

### 1. `ocr_benchmark.json` - Datos Estructurados

```json
{
  "metadata": {
    "timestamp": "2025-01-03 16:30:45",
    "pdf_path": "tests/data/capital414/Capital414_presentacion.pdf",
    "pages_tested": [1, 2, 3]
  },
  "results": {
    "Tesseract Local": {
      "success": true,
      "pages": [...],
      "total_duration_ms": 5432.1,
      "total_chars": 3212,
      "total_words": 492
    },
    "Saptiva OCR": {
      "success": true,
      "pages": [...],
      "total_duration_ms": 12045.7,
      "total_chars": 20529,
      "total_words": 3201
    },
    "DeepSeek OCR": {
      "success": false,
      "pages": [...],
      "total_duration_ms": 145320.5,
      "total_chars": 72,
      "total_words": 12,
      "error": "Timeout on page 2"
    }
  },
  "comparison": {
    "text_similarities": {
      "Tesseract Local_vs_Saptiva OCR": 15.3,
      "Saptiva OCR_vs_DeepSeek OCR": 87.4
    },
    "best_speed": {
      "strategy": "Tesseract Local",
      "duration_ms": 5432.1
    },
    "best_extraction": {
      "strategy": "Saptiva OCR",
      "total_chars": 20529
    }
  }
}
```

### 2. `ocr_benchmark.md` - Reporte Visual

Ejemplo de reporte generado:

```markdown
# ğŸ“Š OCR Benchmark Report

**Generated**: 2025-01-03 16:30:45
**PDF**: Capital414_presentacion.pdf
**Pages Tested**: [1, 2, 3]

## Performance Summary

| Strategy | Success Rate | Avg Duration | Total Chars | Total Words | Status |
|----------|--------------|--------------|-------------|-------------|--------|
| Tesseract Local | 3/3 (100.0%) | 1810.7ms | 3,212 | 492 | âœ… Success |
| Saptiva OCR | 3/3 (100.0%) | 4015.2ms | 20,529 | 3,201 | âœ… Success |
| DeepSeek OCR | 1/3 (33.3%) | 48440.2ms | 72 | 12 | âŒ Failed (Timeout) |

## Text Similarity (Accuracy)

| Comparison | Similarity |
|------------|------------|
| Tesseract Local vs Saptiva OCR | 15.3% |
| Saptiva OCR vs DeepSeek OCR | 87.4% |

## ğŸ† Best Performers

**Fastest**: Tesseract Local (5432ms total)
**Most Text Extracted**: Saptiva OCR (20,529 chars)

## ğŸ’¡ Recommendation

âœ… **MAINTAIN Saptiva OCR** - Balanced performance and reliability
```

## ğŸ“ˆ MÃ©tricas Evaluadas

### Por Estrategia

| MÃ©trica | DescripciÃ³n |
|---------|-------------|
| **Success Rate** | Porcentaje de pÃ¡ginas procesadas exitosamente |
| **Avg Duration** | Tiempo promedio por pÃ¡gina (ms) |
| **Total Chars** | Caracteres totales extraÃ­dos |
| **Total Words** | Palabras totales extraÃ­das |
| **Status** | Estado general (Success/Failed) |

### ComparaciÃ³n entre Estrategias

| MÃ©trica | DescripciÃ³n |
|---------|-------------|
| **Text Similarity** | Similitud textual usando difflib (0-100%) |
| **Best Speed** | Estrategia mÃ¡s rÃ¡pida |
| **Best Extraction** | Estrategia que extrae mÃ¡s texto |

## ğŸ” InterpretaciÃ³n de Resultados

### Criterios de EvaluaciÃ³n

1. **Confiabilidad** (mÃ¡s importante)
   - Success Rate > 95% âœ…
   - Success Rate 80-95% âš ï¸
   - Success Rate < 80% âŒ

2. **Velocidad** (importante para UX)
   - < 5s por pÃ¡gina: Excelente âš¡
   - 5-15s por pÃ¡gina: Aceptable ğŸ‘
   - \> 15s por pÃ¡gina: Lento ğŸŒ

3. **Calidad de ExtracciÃ³n** (crÃ­tico para RAG)
   - Text Similarity > 85%: Alta precisiÃ³n ğŸ¯
   - Text Similarity 70-85%: Aceptable âš ï¸
   - Text Similarity < 70%: Baja precisiÃ³n âŒ

4. **Completitud**
   - Total Chars diferencia < 10%: Equivalente âœ…
   - Total Chars diferencia 10-30%: Revisar ğŸ”
   - Total Chars diferencia > 30%: Problema serio âŒ

### Casos de DecisiÃ³n

#### âœ… Caso 1: Estrategia Dominante
```
Estrategia A: 100% Ã©xito, 4s/pÃ¡gina, 20K chars
Estrategia B: 50% Ã©xito, 8s/pÃ¡gina, 10K chars
â†’ DECISION: Usar Estrategia A
```

#### âš–ï¸ Caso 2: Trade-off Velocidad vs Calidad
```
Estrategia A: 100% Ã©xito, 2s/pÃ¡gina, 15K chars
Estrategia B: 100% Ã©xito, 10s/pÃ¡gina, 20K chars (+33% texto)
â†’ DECISION: Evaluar requisitos
   - Si UX es prioridad â†’ Estrategia A
   - Si precisiÃ³n RAG es prioridad â†’ Estrategia B
```

#### ğŸ” Caso 3: Baja Similitud Textual
```
Estrategia A: 100% Ã©xito, texto legible
Estrategia B: 100% Ã©xito, texto corrupto (15% similitud)
â†’ DECISION: Investigar causa de corrupciÃ³n
   - Verificar manualmente archivos de texto
   - Puede ser problema de encoding, orientaciÃ³n, etc.
```

## ğŸ› ï¸ Troubleshooting

### Error: "Missing dependencies: pytesseract"

```bash
# Instalar Tesseract en el contenedor
docker exec octavios-api apt-get update && apt-get install -y tesseract-ocr tesseract-ocr-spa
```

### Error: "DeepSeek OCR: 401 Unauthorized"

```bash
# Verificar token en .env
grep HF_TOKEN envs/.env

# Si estÃ¡ mal, actualizar y recargar
echo "HF_TOKEN=your_huggingface_token_here" >> envs/.env
make reload-env-service SERVICE=api
```

### Error: "PDF not found"

```bash
# Verificar path del PDF
ls -la tests/data/capital414/*.pdf

# Usar path absoluto si es necesario
python -m tests.test_ocr_benchmark --pdf /full/path/to/file.pdf
```

### Benchmark Muy Lento

```bash
# Probar solo 1-2 pÃ¡ginas primero
make test-benchmark PAGES=2

# Deshabilitar DeepSeek si tiene timeouts
# (remover HF_OCR_ENDPOINT y HF_TOKEN de .env temporalmente)
```

## ğŸ“ Casos de Uso Reales

### 1. ValidaciÃ³n de Nueva Estrategia

```bash
# Escenario: Evaluar si DeepSeek mejora sobre Saptiva OCR
make test-benchmark

# AnÃ¡lisis:
# - Comparar Success Rate (debe ser >= Saptiva)
# - Comparar Text Similarity (debe ser > 85%)
# - Verificar velocidad aceptable (< 10s/pÃ¡gina)
# - Revisar manualmente archivos .txt generados
```

### 2. RegresiÃ³n de Calidad

```bash
# Escenario: Verificar que cambios no degradaron OCR
make test-benchmark

# Guardar baseline:
cp tests/reports/ocr_benchmark.json tests/reports/baseline_2025-01-03.json

# DespuÃ©s de cambios, comparar:
diff tests/reports/baseline_2025-01-03.json tests/reports/ocr_benchmark.json
```

### 3. OptimizaciÃ³n de Costos

```bash
# Escenario: Decidir si vale la pena API externa vs local
make test-benchmark

# AnÃ¡lisis:
# - Calcular costo por pÃ¡gina (API externa)
# - Comparar con costo de infraestructura local
# - Factor de calidad: Â¿justifica el costo?
```

## ğŸ”„ IntegraciÃ³n con CI/CD

### GitHub Actions

```yaml
name: OCR Benchmark

on:
  schedule:
    - cron: '0 0 * * 0'  # Semanal
  workflow_dispatch:     # Manual

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run OCR Benchmark
        run: make test-benchmark
      - name: Upload Reports
        uses: actions/upload-artifact@v3
        with:
          name: ocr-benchmark-reports
          path: tests/reports/
```

## ğŸ“š Recursos Adicionales

- **DocumentaciÃ³n Principal**: `CLAUDE.md` - SecciÃ³n "Document Extraction Abstraction"
- **ImplementaciÃ³n**: `apps/api/src/services/document_extraction.py`
- **Tests Existentes**: `apps/api/tests/unit/test_extractors.py`
- **ComparaciÃ³n Simple**: `tests/ocr_compare_simple.py` (versiÃ³n standalone)

## ğŸ¤ Contribuir

Para aÃ±adir nuevas estrategias al benchmark:

1. Crear clase que herede de `OCRStrategy`
2. Implementar mÃ©todo `extract_pages()`
3. AÃ±adir instancia en `OCRBenchmark.run()`
4. Actualizar esta documentaciÃ³n

Ejemplo:

```python
class MyCustomOCRStrategy(OCRStrategy):
    def __init__(self):
        super().__init__("My Custom OCR")

    async def extract_pages(self, pdf_path: Path, page_numbers: List[int]) -> Dict[str, Any]:
        # Tu implementaciÃ³n aquÃ­
        pass
```

---

**Ãšltima actualizaciÃ³n**: 2025-01-03
**VersiÃ³n**: 1.0.0
**Mantenedor**: Equipo Saptiva
