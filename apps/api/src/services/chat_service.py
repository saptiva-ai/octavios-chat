"""
Chat Service - Business Logic Layer

Extracts business logic from chat router for better separation of concerns.
Handles chat session management, message processing, and AI response generation.
"""

import time
from datetime import datetime
from typing import Dict, List, Optional, Any

import structlog
from fastapi import HTTPException, status

from ..core.config import Settings
from ..core.redis_cache import get_redis_cache
from ..models.chat import ChatSession as ChatSessionModel, ChatMessage as ChatMessageModel, MessageRole
from ..services.saptiva_client import SaptivaClient, build_payload
from ..services.tools import normalize_tools_state
from ..core.telemetry import trace_span

logger = structlog.get_logger(__name__)


class ChatService:
    """Service for chat operations"""

    def __init__(self, settings: Settings):
        self.settings = settings
        self.saptiva_client = SaptivaClient()

    async def get_or_create_session(
        self,
        chat_id: Optional[str],
        user_id: str,
        first_message: str,
        tools_enabled: Dict[str, bool]
    ) -> ChatSessionModel:
        """
        Get existing chat session or create new one.

        Args:
            chat_id: Optional existing chat ID
            user_id: User ID
            first_message: First message content (for new session title)
            tools_enabled: Tools configuration

        Returns:
            ChatSession model

        Raises:
            HTTPException: If chat not found or access denied
        """
        tools_map = normalize_tools_state(tools_enabled)

        if chat_id:
            # Get existing session
            chat_session = await ChatSessionModel.get(chat_id)
            if not chat_session:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="Chat session not found"
                )

            if chat_session.user_id != user_id:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail="Access denied to chat session"
                )

            # Update tools if changed
            existing_tools = normalize_tools_state(getattr(chat_session, 'tools_enabled', None))
            if existing_tools != tools_map:
                await chat_session.update({"$set": {
                    "tools_enabled": tools_map,
                    "updated_at": datetime.utcnow()
                }})
                chat_session.tools_enabled = tools_map

            return chat_session
        else:
            # Create new session
            title = first_message[:50] + "..." if len(first_message) > 50 else first_message
            chat_session = ChatSessionModel(
                title=title,
                user_id=user_id,
                tools_enabled=tools_map
            )
            await chat_session.insert()
            logger.info("Created new chat session", chat_id=chat_session.id, user_id=user_id)
            return chat_session

    async def build_message_context(
        self,
        chat_session: ChatSessionModel,
        current_message: str,
        provided_context: Optional[List[Dict]] = None
    ) -> List[Dict[str, str]]:
        """
        Build conversation context for AI.

        Args:
            chat_session: Chat session
            current_message: Current user message
            provided_context: Optional explicit context

        Returns:
            List of message dicts with role and content
        """
        message_history = []

        if provided_context and len(provided_context) > 0:
            # Use provided context
            for ctx_msg in provided_context:
                message_history.append({
                    "role": ctx_msg.get("role", "user"),
                    "content": ctx_msg.get("content", "")
                })
        else:
            # Get recent messages from session
            recent_messages = await ChatMessageModel.find(
                ChatMessageModel.chat_id == chat_session.id
            ).sort(-ChatMessageModel.created_at).limit(10).to_list()

            # Reverse to chronological order
            for msg in reversed(recent_messages):
                message_history.append({
                    "role": msg.role.value,
                    "content": msg.content
                })

        # Add current message
        message_history.append({
            "role": "user",
            "content": current_message
        })

        return message_history

    async def process_with_saptiva(
        self,
        message: str,
        model: str,
        user_id: str,
        chat_id: str,
        tools_enabled: Dict[str, bool],
        channel: str = "chat",
        user_context: Optional[Dict] = None,
        document_context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Process message using Saptiva (kill switch path).

        Args:
            message: User message
            model: Model name
            user_id: User ID
            chat_id: Chat ID
            tools_enabled: Tools configuration
            channel: Chat channel
            user_context: Additional context
            document_context: Document content for RAG (formatted string)

        Returns:
            Coordinated response format
        """
        async with trace_span(
            "saptiva_simple_chat",
            {
                "chat.id": chat_id,
                "chat.message_length": len(message),
                "chat.model": model,
                "chat.channel": channel
            }
        ):
            # Build context
            context = user_context or {}
            context['chat_id'] = chat_id
            context['user_id'] = user_id

            # Build payload using prompt registry
            payload_data, metadata = build_payload(
                model=model,
                user_prompt=message,
                user_context=context,
                tools_enabled=tools_enabled,
                channel=channel
            )

            # Add document context as system message if provided
            if document_context:
                # Check if context contains images (identified by 游닝 emoji)
                has_images = "游닝" in document_context
                has_pdfs = "游늯" in document_context

                if has_images and not has_pdfs:
                    # Only images - be very explicit
                    system_prompt = (
                        f"El usuario ha adjuntado una o m치s IM츼GENES. "
                        f"Tienes acceso al TEXTO EXTRA칈DO de estas im치genes mediante OCR (reconocimiento 칩ptico de caracteres). "
                        f"IMPORTANTE: Aunque no puedes 'ver' las im치genes, S칈 puedes analizar, leer y responder preguntas sobre el texto que contienen.\n\n"
                        f"Contenido de las im치genes:\n\n{document_context}\n\n"
                        f"Usa esta informaci칩n para responder las preguntas del usuario sobre las im치genes."
                    )
                elif has_images and has_pdfs:
                    # Mixed content
                    system_prompt = (
                        f"El usuario ha adjuntado documentos (PDFs e im치genes). "
                        f"Para las im치genes, tienes el texto extra칤do con OCR. "
                        f"Usa toda esta informaci칩n para responder las preguntas:\n\n{document_context}"
                    )
                else:
                    # Only PDFs - original prompt
                    system_prompt = (
                        f"El usuario ha adjuntado documentos para tu referencia. "
                        f"Usa esta informaci칩n para responder sus preguntas:\n\n{document_context}"
                    )

                system_message = {
                    "role": "system",
                    "content": system_prompt
                }

                # Insert after the main system prompt (usually first message)
                if payload_data["messages"] and payload_data["messages"][0]["role"] == "system":
                    payload_data["messages"].insert(1, system_message)
                else:
                    payload_data["messages"].insert(0, system_message)

                logger.info(
                    "Added document context to prompt",
                    context_length=len(document_context),
                    has_images=has_images,
                    has_pdfs=has_pdfs,
                    chat_id=chat_id
                )

            # Log telemetry
            logger.info(
                "Saptiva request metadata",
                request_id=metadata.get("request_id"),
                system_hash=metadata.get("system_hash"),
                prompt_version=metadata.get("prompt_version"),
                model=model,
                channel=channel,
                has_tools=metadata.get("has_tools", False)
            )

            # Call Saptiva
            start_time = time.time()
            saptiva_response = await self.saptiva_client.chat_completion(
                messages=payload_data["messages"],
                model=model,
                temperature=payload_data.get("temperature", 0.7),
                max_tokens=payload_data.get("max_tokens", 1024),
                stream=False,
                tools=payload_data.get("tools")
            )

            # Format as coordinated response
            return {
                "type": "chat",
                "response": saptiva_response,
                "decision": {
                    "complexity": {"score": 0.0, "requires_research": False},
                    "reason": "Kill switch active - simple inference only"
                },
                "escalation_available": False,
                "processing_time_ms": (time.time() - start_time) * 1000,
                "_metadata": metadata
            }

    async def add_user_message(
        self,
        chat_session: ChatSessionModel,
        content: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> ChatMessageModel:
        """
        Add user message to session with explicit file metadata validation.

        Uses Pydantic models for type-safe metadata instead of Dict[str, Any].

        Args:
            chat_session: Chat session model
            content: Message content
            metadata: Optional metadata dict with 'file_ids' and 'files' keys

        Returns:
            Created ChatMessage with validated file metadata

        Raises:
            ValidationError: If file metadata structure is invalid
            Exception: If database insertion fails
        """
        from pydantic import ValidationError
        from fastapi.encoders import jsonable_encoder
        from ..models.chat import FileMetadata

        try:
            # Extract and validate file metadata using explicit Pydantic models
            file_ids = []
            files = []

            if metadata:
                # Extract file_ids (list of strings)
                file_ids = metadata.get("file_ids", [])

                # Validate and parse files using FileMetadata model
                raw_files = metadata.get("files", [])
                if raw_files:
                    try:
                        files = [FileMetadata.model_validate(f) for f in raw_files]
                        logger.debug(
                            "Validated file metadata",
                            chat_id=chat_session.id,
                            file_count=len(files),
                            filenames=[f.filename for f in files]
                        )
                    except ValidationError as ve:
                        logger.error(
                            "File metadata validation failed (Pydantic)",
                            chat_id=chat_session.id,
                            validation_errors=ve.errors(),
                            raw_files_preview=str(raw_files)[:200],
                            exc_info=True
                        )
                        # Fallback: save only file_ids without rich metadata
                        files = []

            # FIX ISSUE-003: Clean metadata to prevent duplication
            # Remove file_ids and files from metadata since they're stored in explicit fields
            clean_metadata = {"source": "api"}
            if metadata:
                clean_metadata = {
                    k: v for k, v in metadata.items()
                    if k not in ("file_ids", "files")
                }
                clean_metadata["source"] = "api"

            # Create message with explicit typed fields
            user_message = ChatMessageModel(
                chat_id=chat_session.id,
                role=MessageRole.USER,
                content=content,
                file_ids=file_ids,
                files=files,
                schema_version=2,
                # Legacy metadata for backwards compatibility (cleaned)
                metadata=clean_metadata
            )

            # Ensure BSON/JSON serializability before insertion
            try:
                _probe = jsonable_encoder(user_message, by_alias=True)
                logger.debug(
                    "Message is serializable",
                    chat_id=chat_session.id,
                    file_ids_count=len(file_ids),
                    files_count=len(files)
                )
            except Exception as enc_err:
                logger.error(
                    "Message encoding failed (BSON compatibility)",
                    error=str(enc_err),
                    chat_id=chat_session.id,
                    exc_info=True
                )
                raise

            # Persist to MongoDB
            await user_message.insert()

            # Update session stats (message_count, timestamps)
            chat_session.message_count += 1
            chat_session.updated_at = datetime.utcnow()
            chat_session.last_message_at = datetime.utcnow()
            if chat_session.message_count == 1:
                chat_session.first_message_at = datetime.utcnow()
            await chat_session.save()

            logger.info(
                "Added user message with validated files",
                message_id=user_message.id,
                chat_id=chat_session.id,
                file_count=len(files),
                schema_version=2
            )

            # CRITICAL: Record in unified history (so message appears after refresh)
            try:
                from ..services.history_service import HistoryService
                await HistoryService.record_chat_message(
                    chat_id=chat_session.id,
                    user_id=chat_session.user_id,
                    message=user_message
                )
                logger.debug(
                    "Recorded user message in history",
                    message_id=user_message.id,
                    chat_id=chat_session.id
                )
            except Exception as hist_err:
                # Don't fail message creation if history fails, but log it
                logger.error(
                    "Failed to record user message in history",
                    error=str(hist_err),
                    message_id=user_message.id,
                    chat_id=chat_session.id,
                    exc_info=True
                )

            # Invalidate cache
            cache = await get_redis_cache()
            await cache.invalidate_chat_history(chat_session.id)

            return user_message

        except ValidationError as ve:
            logger.error(
                "Pydantic validation failed for user message",
                error=str(ve),
                validation_errors=ve.errors(),
                chat_id=chat_session.id,
                exc_info=True
            )
            raise

        except Exception as e:
            logger.error(
                "Failed to add user message (database or unknown error)",
                error=str(e),
                error_type=type(e).__name__,
                chat_id=chat_session.id,
                has_metadata=bool(metadata),
                exc_info=True
            )
            raise

    async def add_assistant_message(
        self,
        chat_session: ChatSessionModel,
        content: str,
        model: str,
        task_id: Optional[str] = None,
        metadata: Optional[Dict] = None,
        tokens: Optional[Dict] = None,
        latency_ms: Optional[int] = None
    ) -> ChatMessageModel:
        """Add assistant message to session and record in unified history."""
        # FIX ISSUE-003: Clean metadata to prevent duplication (defensive)
        # Although assistant messages typically don't have file_ids, clean metadata to be safe
        clean_metadata = {}
        if metadata:
            clean_metadata = {
                k: v for k, v in metadata.items()
                if k not in ("file_ids", "files")
            }

        ai_message = await chat_session.add_message(
            role=MessageRole.ASSISTANT,
            content=content,
            model=model,
            task_id=task_id,
            metadata=clean_metadata,
            tokens=tokens,
            latency_ms=latency_ms
        )

        # NOTE: History recording is handled by chat_session.add_message() (chat.py:236)
        # No need to call HistoryService.record_chat_message() here

        # Invalidate cache
        cache = await get_redis_cache()
        await cache.invalidate_chat_history(chat_session.id)
        if task_id:
            await cache.invalidate_research_tasks(chat_session.id)

        return ai_message
