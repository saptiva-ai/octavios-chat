## ğŸ“Š MCP Performance & Load Testing

DocumentaciÃ³n completa para tests de performance, carga y stress de las herramientas MCP.

---

## ğŸ¯ Objetivos

Los tests de performance nos ayudan a:

1. **Medir rendimiento**: Tiempo de respuesta, throughput, latencia
2. **Detectar cuellos de botella**: Identificar componentes lentos
3. **Validar escalabilidad**: Verificar comportamiento bajo carga
4. **Monitorear regresiones**: Comparar con baseline histÃ³rico
5. **Optimizar caching**: Medir efectividad del cachÃ©
6. **Stress testing**: Encontrar lÃ­mites del sistema

---

## ğŸ“ Estructura

```
tests/performance/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                    # ConfiguraciÃ³n y fixtures de performance
â”œâ”€â”€ test_mcp_performance.py        # Tests de performance MCP (680 lÃ­neas)
â””â”€â”€ README_PERFORMANCE.md          # Esta documentaciÃ³n
```

---

## ğŸ§ª Tipos de Tests

### 1. Response Time Tests (Single Request)

Mide el tiempo de respuesta de invocaciones individuales.

**Tests incluidos**:
- `test_deep_research_response_time` - MediciÃ³n con 10 rondas
- `test_extract_document_text_response_time_cached` - Con cache hit
- `test_extract_document_text_response_time_extraction` - Con extracciÃ³n real

**MÃ©tricas**:
- â±ï¸ Tiempo promedio de respuesta
- ğŸ“Š Min/Max latencia
- ğŸ”„ Rounds ejecutados

### 2. Concurrent Load Tests

Prueba el sistema con mÃºltiples peticiones simultÃ¡neas.

**Tests incluidos**:
- `test_concurrent_document_extraction_10_users` - 10 usuarios concurrentes
- `test_concurrent_deep_research_5_users` - 5 usuarios concurrentes
- `test_sustained_load_30_requests` - 30 requests secuenciales

**MÃ©tricas**:
- ğŸš€ Throughput (req/s)
- â±ï¸ Latencia p50, p95, p99
- âœ… Tasa de Ã©xito
- â³ DuraciÃ³n total

### 3. Stress Tests (Heavy Load)

Empuja el sistema a sus lÃ­mites para identificar puntos de quiebre.

**Tests incluidos** (marker `slow`):
- `test_stress_burst_50_concurrent_requests` - 50 requests simultÃ¡neos
- `test_stress_sustained_100_requests_over_time` - 100 requests en batches

**MÃ©tricas**:
- ğŸ’¥ Tasa de fallo
- ğŸ”¥ Throughput bajo presiÃ³n
- ğŸ“ˆ Latencia p99
- ğŸ¯ Umbral de Ã©xito (â‰¥80%)

### 4. Cache Performance Tests

Compara rendimiento con y sin cachÃ©.

**Tests incluidos**:
- `test_cache_hit_vs_miss_performance` - Speedup del cachÃ©

**MÃ©tricas**:
- ğŸ’¾ Tiempo cache hit vs miss
- âš¡ Speedup factor (ej: 3.5x)
- ğŸ“Š Efectividad del cachÃ©

---

## ğŸš€ EjecuciÃ³n de Tests

### Comandos Makefile (Recomendado)

```bash
# Ejecutar todos los tests de performance (excluye stress tests)
make test-mcp-performance

# Ejecutar con benchmarks detallados
make test-mcp-performance-benchmark

# Ejecutar stress tests (lentos)
make test-mcp-stress

# Guardar baseline de performance
make test-mcp-performance-save-baseline

# Comparar contra baseline
make test-mcp-performance-compare
```

### Pytest Directo

```bash
# Todos los tests de performance
pytest tests/performance/test_mcp_performance.py -v -m performance

# Solo benchmarks (sin stress tests)
pytest tests/performance/test_mcp_performance.py -m "performance and not slow"

# Solo stress tests
pytest tests/performance/test_mcp_performance.py -m "performance and slow"

# Con output detallado
pytest tests/performance/test_mcp_performance.py -v -s -m performance

# Specific test class
pytest tests/performance/test_mcp_performance.py::TestMCPToolsResponseTime -v

# Specific test
pytest tests/performance/test_mcp_performance.py::TestMCPToolsResponseTime::test_deep_research_response_time -v
```

---

## ğŸ“Š Interpretar Resultados

### Output Example

```
ğŸ“Š Concurrent Load (10 users):
  Total Duration: 2.45s
  Throughput: 4.08 req/s
  Avg Latency: 0.245s

ğŸ“Š Sustained Load (30 requests):
  Total Duration: 5.12s
  Throughput: 5.86 req/s
  Success Rate: 100.0%
  Latency p50: 0.165s
  Latency p95: 0.298s
  Latency p99: 0.312s

ğŸ”¥ Stress Test (50 concurrent):
  Total Duration: 8.34s
  Throughput: 5.99 req/s
  Successful: 48/50 (96.0%)
  Failed: 2/50
  Latency p50: 1.234s
  Latency p95: 2.567s
  Latency p99: 3.012s

ğŸ’¾ Cache Performance:
  Cache Hit (avg): 0.123s
  Cache Miss (avg): 0.456s
  Speedup: 3.7x
```

### MÃ©tricas Clave

| MÃ©trica | Bueno | Aceptable | Malo |
|---------|-------|-----------|------|
| **p50 Latency** | < 0.2s | < 0.5s | > 1s |
| **p95 Latency** | < 0.5s | < 1s | > 2s |
| **p99 Latency** | < 1s | < 2s | > 5s |
| **Throughput** | > 10 req/s | > 5 req/s | < 2 req/s |
| **Success Rate** | 100% | > 95% | < 90% |
| **Cache Speedup** | > 3x | > 2x | < 1.5x |

### InterpretaciÃ³n

**âœ… Performance Ã“ptimo**:
- p95 < 500ms
- Throughput > 10 req/s
- Success rate 100%
- Cache speedup > 3x

**âš ï¸ Performance Aceptable**:
- p95 < 1s
- Throughput > 5 req/s
- Success rate > 95%
- Cache speedup > 2x

**âŒ Performance ProblemÃ¡tico**:
- p95 > 2s
- Throughput < 2 req/s
- Success rate < 90%
- Cache speedup < 1.5x

---

## ğŸ”§ ConfiguraciÃ³n

### pytest-benchmark

Tests usan `pytest-benchmark` para mediciones precisas:

```python
async def test_performance(self, benchmark):
    async def make_request():
        # cÃ³digo a medir
        ...

    result = await benchmark.pedantic(
        make_request,
        rounds=10,        # 10 rondas de mediciÃ³n
        iterations=1      # 1 iteraciÃ³n por ronda
    )
```

### ConfiguraciÃ³n en conftest.py

```python
benchmark_config = {
    "min_rounds": 5,
    "min_time": 0.1,
    "max_time": 5.0,
    "warmup": True,
    "warmup_iterations": 2,
}
```

### Resource Monitoring

Tests incluyen monitoreo de recursos:

```python
def test_with_monitoring(self, resource_monitor):
    # cÃ³digo del test
    ...

    # AutomÃ¡ticamente imprime al final:
    # ğŸ“Š Resource Usage:
    #   Memory: 245.3 MB (+12.5 MB)
    #   CPU: 45.2%
```

---

## ğŸ“ˆ Benchmark Management

### Guardar Baseline

```bash
# Primera vez: establecer baseline
make test-mcp-performance-save-baseline
```

Esto crea `.benchmarks/baseline.json` con mÃ©tricas actuales.

### Comparar con Baseline

```bash
# DespuÃ©s de cambios: comparar performance
make test-mcp-performance-compare
```

**Falla si**:
- Mean performance regresa > 10%
- Cualquier mÃ©trica empeora significativamente

### Ver Historial

```bash
# Listar todos los benchmarks guardados
ls -la apps/api/.benchmarks/

# Ver benchmark especÃ­fico
cat apps/api/.benchmarks/baseline.json
```

---

## ğŸ¯ Assertions en Tests

### Response Time Tests

```python
# Benchmark automÃ¡tico (no assertions manuales)
result = await benchmark.pedantic(make_request, rounds=10, iterations=1)
```

### Concurrent Load Tests

```python
assert successful == 10, f"Only {successful}/10 requests succeeded"
assert duration < 5.0, f"10 concurrent requests took {duration:.2f}s (should be < 5s)"
assert throughput > 2.0, f"Throughput {throughput:.2f} req/s is too low"
```

### Stress Tests

```python
assert successful >= 40, f"Only {successful}/50 requests succeeded (< 80%)"
assert p95 < 5.0, f"p95 latency {p95:.3f}s is extremely high"
```

### Cache Tests

```python
assert speedup > 1.2, f"Cache only provides {speedup:.1f}x speedup (should be > 1.2x)"
```

---

## ğŸ› Troubleshooting

### Tests muy lentos

**Problema**: Tests tardan demasiado

**SoluciÃ³n**:
```bash
# Excluir stress tests (marker slow)
pytest tests/performance -m "performance and not slow"

# Reducir rounds en benchmark
# Editar conftest.py: min_rounds = 3
```

### Variabilidad alta en mÃ©tricas

**Problema**: Resultados inconsistentes entre runs

**Causas**:
- Otros procesos en el sistema
- Docker compartiendo recursos
- ConexiÃ³n de red inestable

**SoluciÃ³n**:
```bash
# Cerrar aplicaciones innecesarias
# Ejecutar mÃºltiples veces y promediar
for i in {1..5}; do
  make test-mcp-performance
done

# Aumentar warmup iterations
# En conftest.py: warmup_iterations = 5
```

### Benchmarks fallan al comparar

**Problema**: `--benchmark-compare` falla

**SoluciÃ³n**:
```bash
# Verificar que baseline existe
ls -la apps/api/.benchmarks/baseline.json

# Recrear baseline si es necesario
make test-mcp-performance-save-baseline

# Ajustar umbral de falla
pytest ... --benchmark-compare-fail=mean:20%  # 20% en lugar de 10%
```

### Memory leaks

**Problema**: Uso de memoria crece durante tests

**DiagnÃ³stico**:
```python
def test_with_monitoring(self, resource_monitor):
    # El fixture imprime memory delta al final
    ...

# Output:
# ğŸ“Š Resource Usage:
#   Memory: 456.7 MB (+125.3 MB)  # âš ï¸ +125MB es mucho
```

**SoluciÃ³n**:
- Revisar fixtures que no limpian recursos
- Verificar conexiones de DB/Redis cerradas
- Usar `gc.collect()` explÃ­cito si es necesario

---

## ğŸ”¬ Agregar Nuevos Tests

### 1. Test de Response Time

```python
@pytest.mark.performance
@pytest.mark.asyncio
class TestNewToolResponseTime:
    async def test_new_tool_response_time(
        self,
        client: AsyncClient,
        perf_user_with_token,
        benchmark
    ):
        """Measure new_tool response time."""
        access_token, user_id = perf_user_with_token

        payload = {
            "tool": "new_tool",
            "payload": {"param": "value"}
        }

        async def make_request():
            response = await client.post(
                "/api/mcp/tools/invoke",
                json=payload,
                headers={"Authorization": f"Bearer {access_token}"}
            )
            return response

        result = await benchmark.pedantic(
            make_request,
            rounds=10,
            iterations=1
        )

        assert result.status_code == 200
```

### 2. Test de Carga Concurrente

```python
async def test_concurrent_new_tool_20_users(
    self,
    client: AsyncClient,
    perf_user_with_token
):
    """Test 20 concurrent requests."""
    access_token, user_id = perf_user_with_token

    payload = {"tool": "new_tool", "payload": {}}

    async def make_request():
        response = await client.post(
            "/api/mcp/tools/invoke",
            json=payload,
            headers={"Authorization": f"Bearer {access_token}"}
        )
        return response.status_code, time.time()

    start_time = time.time()
    tasks = [make_request() for _ in range(20)]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    duration = time.time() - start_time

    successful = sum(1 for r in results if not isinstance(r, Exception) and r[0] == 200)
    throughput = 20 / duration

    print(f"\nğŸ“Š New Tool Concurrent (20 users):")
    print(f"  Duration: {duration:.2f}s")
    print(f"  Throughput: {throughput:.2f} req/s")
    print(f"  Success: {successful}/20")

    assert successful == 20
    assert throughput > 3.0
```

### 3. Test de Stress

```python
@pytest.mark.slow
async def test_stress_new_tool_100_requests(
    self,
    client: AsyncClient,
    perf_user_with_token
):
    """Stress test with 100 concurrent requests."""
    # Similar a concurrent pero con 100 requests
    # Assertions mÃ¡s permisivas (success >= 80%)
```

---

## ğŸ“š Best Practices

### âœ… DO

1. **Usar fixtures de performance**: `perf_user_with_token`, `perf_document_pdf`
2. **Mock servicios externos**: Aletheia, MinIO, etc
3. **Medir mÃ©tricas relevantes**: p50, p95, p99, throughput
4. **Documentar thresholds**: Assertions con mensajes claros
5. **Imprimir resultados**: `print(f"\nğŸ“Š Metrics: ...")` para debugging
6. **Usar markers**: `@pytest.mark.performance`, `@pytest.mark.slow`
7. **Cleanup**: Asegurar que fixtures limpian recursos

### âŒ DON'T

1. **No hardcodear valores**: Usar variables para thresholds
2. **No ignorar variabilidad**: Ejecutar mÃºltiples rounds
3. **No testear solo happy path**: Incluir escenarios de error
4. **No olvidar stress tests**: Identificar lÃ­mites del sistema
5. **No mockear todo**: Performance tests necesitan componentes reales
6. **No commitear benchmarks grandes**: `.benchmarks/` en `.gitignore`

---

## ğŸ“Š MÃ©tricas Monitoreadas

### Latencia (Response Time)

- **p50** (mediana): 50% de requests mÃ¡s rÃ¡pidos
- **p95**: 95% de requests mÃ¡s rÃ¡pidos (outliers excluidos)
- **p99**: 99% de requests mÃ¡s rÃ¡pidos (peor caso tÃ­pico)
- **Max**: Peor caso absoluto

### Throughput

- **req/s**: Requests procesados por segundo
- **Duration**: Tiempo total para N requests
- **Concurrency**: NÃºmero de requests simultÃ¡neos

### Reliability

- **Success Rate**: % de requests exitosos (200 OK)
- **Error Rate**: % de requests fallidos (â‰  200)
- **Timeout Rate**: % de requests que timeout

### Resources

- **Memory**: Uso de RAM durante test
- **Memory Delta**: Incremento desde inicio
- **CPU**: % de CPU utilizado

---

## ğŸ¯ Objetivos de Performance

### Tier 1: Operaciones RÃ¡pidas (cached)
- **Target**: p95 < 200ms
- **Examples**: document_extraction (cached), tools_list

### Tier 2: Operaciones Normales
- **Target**: p95 < 1s
- **Examples**: document_extraction (pypdf), audit_file

### Tier 3: Operaciones Pesadas
- **Target**: p95 < 5s
- **Examples**: deep_research, OCR extraction

### Tier 4: Operaciones Muy Pesadas
- **Target**: p95 < 30s
- **Examples**: deep_research (deep mode), full validation

---

## ğŸ”— Referencias

- [pytest-benchmark docs](https://pytest-benchmark.readthedocs.io/)
- [Locust (load testing)](https://locust.io/)
- [Performance Testing Guide](https://martinfowler.com/articles/practical-test-pyramid.html#PerformanceTests)
- [MCP Architecture](../../../../docs/MCP_ARCHITECTURE.md)
- [Integration Tests](../integration/README_MCP_TESTS.md)
