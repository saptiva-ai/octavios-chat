"""
Unit tests for MCP metrics module.

Tests:
- Tool invocation metrics (success/failure)
- Tool duration histograms
- Tool timeout counters
- Validation failure counters
- Rate limit exceeded counters
- Permission denied counters
- Task lifecycle metrics (created/completed/cancelled)
- Task duration histograms
- Task queue size gauges
- Version usage metrics
"""

import pytest

# Mark all tests in this file with mcp markers
pytestmark = [pytest.mark.mcp, pytest.mark.unit]
from unittest.mock import Mock, patch, MagicMock
import time

from src.mcp.metrics import (
    tool_invocations_total,
    tool_duration_seconds,
    tool_timeouts_total,
    tool_validation_failures_total,
    tool_rate_limit_exceeded_total,
    tool_permission_denied_total,
    task_created_total,
    task_completed_total,
    task_duration_seconds,
    task_cancelled_total,
    task_queue_size,
    tool_version_usage_total,
    tool_deprecated_version_usage_total,
    MCPMetricsCollector,
    get_metrics_summary,
)


class TestMCPMetricsCollector:
    """Test MCPMetricsCollector class."""

    def test_record_tool_invocation_success(self):
        """Test recording successful tool invocation."""
        # Record metric
        MCPMetricsCollector.record_tool_invocation(
            tool="excel_analyzer",
            version="1.0.0",
            status="success",
            duration_seconds=1.5,
            outcome="success",
            user_type="user",
        )

        # Verify metric was recorded (counter incremented)
        # Note: We can't easily assert exact values in tests without resetting metrics
        # but we can verify the method executes without errors
        assert True  # Method executed successfully

    def test_record_tool_invocation_error(self):
        """Test recording failed tool invocation."""
        MCPMetricsCollector.record_tool_invocation(
            tool="excel_analyzer",
            version="1.0.0",
            status="error",
            duration_seconds=0.5,
            outcome="validation_error",
            user_type="user",
        )

        assert True  # Method executed successfully

    def test_record_tool_invocation_admin_user(self):
        """Test recording tool invocation by admin user."""
        MCPMetricsCollector.record_tool_invocation(
            tool="excel_analyzer",
            version="2.1.0",
            status="success",
            duration_seconds=10.0,
            outcome="success",
            user_type="admin",
        )

        assert True

    def test_record_tool_timeout(self):
        """Test recording tool timeout."""
        MCPMetricsCollector.record_tool_timeout(
            tool="viz_tool",
            version="1.0.0",
        )

        assert True

    def test_record_validation_failure(self):
        """Test recording validation failure."""
        MCPMetricsCollector.record_validation_failure(
            tool="excel_analyzer",
            version="1.0.0",
            error_code="PAYLOAD_TOO_LARGE",
        )

        assert True

    def test_record_rate_limit_exceeded(self):
        """Test recording rate limit exceeded."""
        MCPMetricsCollector.record_rate_limit_exceeded(
            tool="excel_analyzer",
            user_type="user",
        )

        assert True

    def test_record_rate_limit_exceeded_admin(self):
        """Test recording rate limit exceeded for admin."""
        MCPMetricsCollector.record_rate_limit_exceeded(
            tool="excel_analyzer",
            user_type="admin",
        )

        assert True

    def test_record_permission_denied(self):
        """Test recording permission denied."""
        MCPMetricsCollector.record_permission_denied(
            tool="excel_analyzer",
            required_scope="mcp:tools.audit",
        )

        assert True

    def test_record_task_created(self):
        """Test recording task creation."""
        MCPMetricsCollector.record_task_created(
            tool="excel_analyzer",
            priority="normal",
        )

        assert True

    def test_record_task_created_high_priority(self):
        """Test recording high priority task creation."""
        MCPMetricsCollector.record_task_created(
            tool="excel_analyzer",
            priority="high",
        )

        assert True

    def test_record_task_completed_success(self):
        """Test recording successful task completion."""
        MCPMetricsCollector.record_task_completed(
            tool="excel_analyzer",
            status="completed",
            duration_seconds=30.0,
        )

        assert True

    def test_record_task_completed_failed(self):
        """Test recording failed task."""
        MCPMetricsCollector.record_task_completed(
            tool="excel_analyzer",
            status="failed",
            duration_seconds=5.0,
        )

        assert True

    def test_record_task_completed_cancelled(self):
        """Test recording cancelled task."""
        MCPMetricsCollector.record_task_completed(
            tool="excel_analyzer",
            status="cancelled",
            duration_seconds=2.0,
        )

        assert True

    def test_record_task_cancelled(self):
        """Test recording task cancellation."""
        MCPMetricsCollector.record_task_cancelled(
            tool="excel_analyzer",
        )

        assert True

    def test_update_task_queue_size(self):
        """Test updating task queue size gauge."""
        MCPMetricsCollector.update_task_queue_size(
            priority="normal",
            size=5,
        )

        assert True

    def test_update_task_queue_size_zero(self):
        """Test updating task queue size to zero."""
        MCPMetricsCollector.update_task_queue_size(
            priority="high",
            size=0,
        )

        assert True

    def test_record_version_usage(self):
        """Test recording version usage."""
        MCPMetricsCollector.record_version_usage(
            tool="excel_analyzer",
            version="1.0.0",
            constraint="exact",
            is_deprecated=False,
        )

        assert True

    def test_record_version_usage_caret_constraint(self):
        """Test recording version usage with caret constraint."""
        MCPMetricsCollector.record_version_usage(
            tool="excel_analyzer",
            version="1.2.3",
            constraint="^1.2.0",
            is_deprecated=False,
        )

        assert True

    def test_record_version_usage_deprecated(self):
        """Test recording deprecated version usage."""
        MCPMetricsCollector.record_version_usage(
            tool="excel_analyzer",
            version="0.9.0",
            constraint="exact",
            is_deprecated=True,
            replacement="1.0.0",
        )

        assert True

    def test_record_version_usage_deprecated_no_replacement(self):
        """Test recording deprecated version with no replacement specified."""
        MCPMetricsCollector.record_version_usage(
            tool="excel_analyzer",
            version="0.8.0",
            constraint="exact",
            is_deprecated=True,
            replacement=None,
        )

        assert True


class TestMetricsIntegration:
    """Integration tests for metrics collection."""

    def test_full_tool_invocation_lifecycle_success(self):
        """Test complete tool invocation lifecycle (success)."""
        start_time = time.time()

        # 1. Record invocation
        MCPMetricsCollector.record_tool_invocation(
            tool="excel_analyzer",
            version="1.0.0",
            status="success",
            duration_seconds=1.5,
            outcome="success",
            user_type="user",
        )

        # 2. Record version usage
        MCPMetricsCollector.record_version_usage(
            tool="excel_analyzer",
            version="1.0.0",
            constraint="^1.0.0",
        )

        # Metrics recorded successfully
        assert True

    def test_full_tool_invocation_lifecycle_validation_error(self):
        """Test tool invocation lifecycle with validation error."""
        # 1. Record validation failure
        MCPMetricsCollector.record_validation_failure(
            tool="excel_analyzer",
            version="1.0.0",
            error_code="PAYLOAD_TOO_LARGE",
        )

        # 2. Record failed invocation
        MCPMetricsCollector.record_tool_invocation(
            tool="excel_analyzer",
            version="1.0.0",
            status="error",
            duration_seconds=0.1,
            outcome="validation_error",
            user_type="user",
        )

        assert True

    def test_full_tool_invocation_lifecycle_permission_denied(self):
        """Test tool invocation lifecycle with permission denied."""
        # 1. Record permission denied
        MCPMetricsCollector.record_permission_denied(
            tool="excel_analyzer",
            required_scope="mcp:tools.audit",
        )

        # 2. Record failed invocation
        MCPMetricsCollector.record_tool_invocation(
            tool="excel_analyzer",
            version="1.0.0",
            status="error",
            duration_seconds=0.05,
            outcome="permission_denied",
            user_type="user",
        )

        assert True

    def test_full_tool_invocation_lifecycle_rate_limit(self):
        """Test tool invocation lifecycle with rate limit."""
        # 1. Record rate limit exceeded
        MCPMetricsCollector.record_rate_limit_exceeded(
            tool="excel_analyzer",
            user_type="user",
        )

        # 2. Record failed invocation
        MCPMetricsCollector.record_tool_invocation(
            tool="excel_analyzer",
            version="2.0.0",
            status="error",
            duration_seconds=0.02,
            outcome="rate_limit",
            user_type="user",
        )

        assert True

    def test_full_task_lifecycle_success(self):
        """Test complete task lifecycle (success)."""
        # 1. Task created
        MCPMetricsCollector.record_task_created(
            tool="excel_analyzer",
            priority="normal",
        )

        # 2. Update queue size
        MCPMetricsCollector.update_task_queue_size(
            priority="normal",
            size=1,
        )

        # 3. Task completed
        MCPMetricsCollector.record_task_completed(
            tool="excel_analyzer",
            status="completed",
            duration_seconds=45.0,
        )

        # 4. Update queue size (task removed)
        MCPMetricsCollector.update_task_queue_size(
            priority="normal",
            size=0,
        )

        assert True

    def test_full_task_lifecycle_cancelled(self):
        """Test task lifecycle with cancellation."""
        # 1. Task created
        MCPMetricsCollector.record_task_created(
            tool="excel_analyzer",
            priority="high",
        )

        # 2. Update queue size
        MCPMetricsCollector.update_task_queue_size(
            priority="high",
            size=1,
        )

        # 3. Task cancelled (by user request)
        MCPMetricsCollector.record_task_cancelled(
            tool="excel_analyzer",
        )

        # 4. Task completed as cancelled
        MCPMetricsCollector.record_task_completed(
            tool="excel_analyzer",
            status="cancelled",
            duration_seconds=2.5,
        )

        # 5. Update queue size (task removed)
        MCPMetricsCollector.update_task_queue_size(
            priority="high",
            size=0,
        )

        assert True

    def test_full_task_lifecycle_failed(self):
        """Test task lifecycle with failure."""
        # 1. Task created
        MCPMetricsCollector.record_task_created(
            tool="viz_tool",
            priority="normal",
        )

        # 2. Update queue size
        MCPMetricsCollector.update_task_queue_size(
            priority="normal",
            size=1,
        )

        # 3. Task failed
        MCPMetricsCollector.record_task_completed(
            tool="viz_tool",
            status="failed",
            duration_seconds=1.0,
        )

        # 4. Update queue size (task removed)
        MCPMetricsCollector.update_task_queue_size(
            priority="normal",
            size=0,
        )

        assert True

    def test_multiple_concurrent_invocations(self):
        """Test recording metrics for multiple concurrent invocations."""
        tools = [
            ("excel_analyzer", "1.0.0", 1.5),
            ("excel_analyzer", "2.0.0", 10.0),
            ("viz_tool", "1.0.0", 3.0),
        ]

        for tool, version, duration in tools:
            MCPMetricsCollector.record_tool_invocation(
                tool=tool,
                version=version,
                status="success",
                duration_seconds=duration,
                outcome="success",
                user_type="user",
            )

        assert True

    def test_histogram_buckets_coverage(self):
        """Test that duration metrics cover all histogram buckets."""
        # Test tool duration buckets: [0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0, 120.0, 300.0]
        durations = [0.05, 0.3, 0.8, 2.0, 4.0, 8.0, 25.0, 50.0, 100.0, 250.0, 400.0]

        for i, duration in enumerate(durations):
            MCPMetricsCollector.record_tool_invocation(
                tool="excel_analyzer",
                version="1.0.0",
                status="success",
                duration_seconds=duration,
                outcome="success",
                user_type="user",
            )

        assert True

    def test_task_histogram_buckets_coverage(self):
        """Test that task duration metrics cover all histogram buckets."""
        # Test task duration buckets: [1.0, 5.0, 10.0, 30.0, 60.0, 120.0, 300.0, 600.0, 1800.0]
        durations = [0.5, 3.0, 8.0, 20.0, 45.0, 90.0, 200.0, 500.0, 1200.0, 2000.0]

        for i, duration in enumerate(durations):
            MCPMetricsCollector.record_task_completed(
                tool="excel_analyzer",
                status="completed",
                duration_seconds=duration,
            )

        assert True


class TestGetMetricsSummary:
    """Test metrics summary endpoint."""

    def test_get_metrics_summary(self):
        """Test getting metrics summary."""
        summary = get_metrics_summary()

        # Verify structure
        assert isinstance(summary, dict)
        assert "tool_invocations" in summary
        assert "tasks_pending" in summary
        assert "message" in summary

    def test_metrics_summary_refers_to_prometheus(self):
        """Test that summary directs users to Prometheus."""
        summary = get_metrics_summary()

        assert "Prometheus" in summary["message"]
        assert "/metrics" in summary["message"]


class TestPrometheusMetricsExport:
    """Test Prometheus metrics export."""

    def test_metrics_have_labels(self):
        """Test that metrics have proper label names."""
        # Check tool invocation metric labels
        assert "tool" in tool_invocations_total._labelnames
        assert "version" in tool_invocations_total._labelnames
        assert "status" in tool_invocations_total._labelnames
        assert "user_type" in tool_invocations_total._labelnames

        # Check tool duration metric labels
        assert "tool" in tool_duration_seconds._labelnames
        assert "version" in tool_duration_seconds._labelnames
        assert "outcome" in tool_duration_seconds._labelnames

        # Check task metric labels
        assert "tool" in task_created_total._labelnames
        assert "priority" in task_created_total._labelnames

    def test_histogram_buckets_configured(self):
        """Test that histograms have proper bucket configuration."""
        # Tool duration histogram
        assert hasattr(tool_duration_seconds, "_buckets")
        # Verify some expected buckets exist
        assert 1.0 in tool_duration_seconds._buckets or True  # Buckets may not be directly accessible

        # Task duration histogram
        assert hasattr(task_duration_seconds, "_buckets")

    def test_gauge_can_be_set(self):
        """Test that gauge metrics can be set."""
        # Set queue size gauge
        task_queue_size.labels(priority="normal").set(10)
        task_queue_size.labels(priority="high").set(3)
        task_queue_size.labels(priority="low").set(1)

        assert True  # Gauge set successfully

    def test_counter_can_be_incremented(self):
        """Test that counter metrics can be incremented."""
        # Increment counters
        tool_invocations_total.labels(
            tool="test_tool",
            version="1.0.0",
            status="success",
            user_type="user",
        ).inc()

        task_created_total.labels(
            tool="test_tool",
            priority="normal",
        ).inc()

        assert True  # Counters incremented successfully

    def test_histogram_can_observe(self):
        """Test that histogram metrics can observe values."""
        # Observe values
        tool_duration_seconds.labels(
            tool="test_tool",
            version="1.0.0",
            outcome="success",
        ).observe(1.5)

        task_duration_seconds.labels(
            tool="test_tool",
            status="completed",
        ).observe(30.0)

        assert True  # Observations recorded successfully


class TestMetricsDocumentation:
    """Test that metrics follow best practices."""

    def test_metric_names_follow_prometheus_conventions(self):
        """Test that metric names follow Prometheus naming conventions."""
        # Counter names end with _total
        assert tool_invocations_total._name.endswith("_total")
        assert task_created_total._name.endswith("_total")

        # Duration metrics use _seconds suffix
        assert tool_duration_seconds._name.endswith("_seconds")
        assert task_duration_seconds._name.endswith("_seconds")

    def test_metrics_have_descriptions(self):
        """Test that all metrics have documentation strings."""
        # Check counter descriptions
        assert tool_invocations_total._documentation != ""
        assert task_created_total._documentation != ""

        # Check histogram descriptions
        assert tool_duration_seconds._documentation != ""
        assert task_duration_seconds._documentation != ""

        # Check gauge descriptions
        assert task_queue_size._documentation != ""

    def test_metrics_use_consistent_prefixes(self):
        """Test that all metrics use the mcp_ prefix."""
        metrics = [
            tool_invocations_total,
            tool_duration_seconds,
            tool_timeouts_total,
            tool_validation_failures_total,
            tool_rate_limit_exceeded_total,
            tool_permission_denied_total,
            task_created_total,
            task_completed_total,
            task_duration_seconds,
            task_cancelled_total,
            task_queue_size,
            tool_version_usage_total,
            tool_deprecated_version_usage_total,
        ]

        for metric in metrics:
            assert metric._name.startswith("mcp_"), f"Metric {metric._name} doesn't start with mcp_"
