# POST-MORTEM: P√©rdida de Datos MongoDB - 2025-10-09

**Fecha del Incidente:** 9 de Octubre, 2025
**Severidad:** CR√çTICA - P√©rdida total de datos hist√≥ricos de producci√≥n
**Tiempo de Detecci√≥n:** ~30 minutos despu√©s del deployment
**Tiempo de Recuperaci√≥n:** Irrecuperable
**Estado Final:** Base de datos reinicializada, datos hist√≥ricos perdidos permanentemente

---

## üìã RESUMEN EJECUTIVO

Durante un deployment de producci√≥n el 9 de octubre de 2025, se perdieron todos los datos hist√≥ricos de MongoDB debido a una combinaci√≥n de:
1. **Configuraci√≥n incorrecta de vol√∫menes** entre entornos dev y prod
2. **Ausencia de backups automatizados** de la base de datos
3. **M√∫ltiples reinicios de MongoDB** durante troubleshooting que sobrescribieron datos
4. **Falta de procedimientos documentados** de backup/restore

---

## üïê TIMELINE DEL INCIDENTE

### **20:00** - Inicio del Deployment
- Deployment iniciado con `make deploy-tar`
- Im√°genes Docker construidas localmente con target de producci√≥n
- Transferencia exitosa de im√°genes v√≠a TAR a servidor

### **20:17** - Primer Error de Autenticaci√≥n MongoDB
```
pymongo.errors.OperationFailure: Authentication failed
Credential mismatch: API buscando credenciales diferentes a las inicializadas
```
**Causa:** docker-compose.yml base ten√≠a hardcoded `env_file: ../envs/.env` (dev) en lugar de `.env.prod`

### **20:25-20:45** - M√∫ltiples Intentos de Fix
- Limpieza de vol√∫menes con `docker compose down -v` **(PUNTO CR√çTICO DE P√âRDIDA)**
- Creaci√≥n de symlinks `.env -> .env.prod`
- Reintentos con diferentes configuraciones
- **Cada `down -v` eliminaba datos y recreaba vol√∫menes vac√≠os**

### **20:46** - Redirecci√≥n a TAR Deployment
- Usuario solicit√≥ enfoque m√°s limpio
- Construcci√≥n de im√°genes locales exitosa
- Problema: Web image usando target dev en lugar de runner

### **21:06** - Fix de Build Targets
- Script de deployment modificado para usar targets correctos:
  - API: `--target production`
  - Web: `--target runner`

### **21:46** - MongoDB Reinitializado
- Al reiniciar servicios, MongoDB cre√≥ base de datos nueva
- Usuario detect√≥ p√©rdida de historial
- **Solo 1 usuario, 1 sesi√≥n, 2 mensajes** (datos de prueba del d√≠a)

### **21:52-22:00** - Intento de Recuperaci√≥n
- Identificaci√≥n de archivos hist√≥ricos en `/opt/copilotos-bridge/data/mongodb/`
- Intento fallido de restauraci√≥n manual
- **Problema:** Archivos sin cat√°logo correcto (`_mdb_catalog.wt`) son irrecuperables

### **22:04** - Aceptaci√≥n y Reinicializaci√≥n
- Decisi√≥n de empezar con base de datos limpia
- Todos los servicios healthy y funcionando

---

## üîç AN√ÅLISIS DE CAUSAS RA√çZ

### **Causa Primaria: Configuraci√≥n de Vol√∫menes Inconsistente**

#### **Problema 1: M√∫ltiples Ubicaciones de Datos**
```yaml
# docker-compose.yml (DEV) - Usaba Docker volumes an√≥nimos
volumes:
  mongodb_data:
    driver: local

# docker-compose.prod.yml - Configurado para bind mounts
volumes:
  - /opt/copilotos-bridge/data/mongodb:/data/db
```

**Impacto:**
- Datos hist√≥ricos en `/opt/copilotos-bridge/data/mongodb/` (nunca usado por prod)
- Datos de prod en `/var/lib/docker/volumes/copilotos-prod_mongodb_data/`
- **Dos sistemas completamente separados**

#### **Problema 2: COMPOSE_PROJECT_NAME Cambi√≥**
```bash
# Original: COMPOSE_PROJECT_NAME=copilotos
# Producci√≥n: COMPOSE_PROJECT_NAME=copilotos-prod

# Resultado: Vol√∫menes diferentes
copilotos_mongodb_data       # Dev (371MB de datos hist√≥ricos)
copilotos-prod_mongodb_data  # Prod (recreado vac√≠o m√∫ltiples veces)
```

### **Causa Secundaria: Ausencia de Backups**

**Estado del Sistema de Backups:**
- ‚ùå No hab√≠a backups automatizados de MongoDB
- ‚ùå No hab√≠a cron jobs configurados
- ‚ùå No hab√≠a snapshots de servidor
- ‚ùå No hab√≠a documentaci√≥n de procedimientos de backup
- ‚úÖ Solo exist√≠a un backup manual de 108 bytes (casi vac√≠o)

### **Causa Terciaria: Comandos Destructivos Durante Troubleshooting**

**Comandos Ejecutados que Causaron P√©rdida:**
```bash
# 1. Limpieza de vol√∫menes (destruye datos)
docker compose down -v

# 2. Recreaci√≥n de vol√∫menes vac√≠os
docker compose up -d

# 3. Remoci√≥n manual de im√°genes antiguas
docker rmi copilotos-web:latest copilotos-api:latest

# 4. M√∫ltiples reinicios de MongoDB
# Cada reinicio sin cat√°logo correcto creaba base de datos nueva
```

**Por qu√© fueron destructivos:**
- `-v` flag elimina vol√∫menes nombrados permanentemente
- Sin backups previos, la eliminaci√≥n fue irrecuperable
- MongoDB sin `_mdb_catalog.wt` correcto no puede leer archivos de colecci√≥n

### **Causa Cuaternaria: Docker Compose Override No Persistente**

El deployment script creaba `docker-compose.override.yml` temporalmente:
```bash
# Deployment script
ssh "$SERVER" "cat > docker-compose.override.yml <<EOF
image: copilotos-web:latest
build: {}
EOF"

# ...despu√©s...
ssh "$SERVER" "rm -f docker-compose.override.yml"
```

**Problema:** Al hacer `docker compose down/up` manual, el override no exist√≠a y Docker volv√≠a a build target dev.

---

## üí• IMPACTO DEL INCIDENTE

### **Datos Perdidos**
- ‚ùå **Todos los usuarios hist√≥ricos** (cantidad desconocida)
- ‚ùå **Todas las sesiones de chat** anteriores a hoy
- ‚ùå **Todos los mensajes** hist√≥ricos
- ‚ùå **Todo el historial de eventos**
- ‚ùå **Todas las fuentes de investigaci√≥n**
- ‚ùå **Todas las tareas** guardadas

### **Datos Preservados**
- ‚úÖ C√≥digo fuente (Git)
- ‚úÖ Configuraci√≥n de infraestructura
- ‚úÖ Archivos de colecci√≥n MongoDB (irrecuperables sin cat√°logo)

### **Impacto en Usuarios**
- Usuario actual (jaziel/jf@saptiva.com) perdi√≥ todo su historial
- Cualquier otro usuario que existiera perdi√≥ su cuenta y datos
- **Necesidad de re-registro** para todos los usuarios

### **Impacto Operacional**
- ~4 horas de downtime/troubleshooting
- P√©rdida de confianza en sistema de deployment
- Urgencia cr√≠tica para implementar backups

---

## üõ°Ô∏è MEDIDAS PREVENTIVAS IMPLEMENTADAS INMEDIATAMENTE

### ‚úÖ **1. Docker Compose Override Permanente**
Creado archivo permanente en servidor:
```yaml
# /home/jf/copilotos-bridge/infra/docker-compose.override.yml
version: '3.8'
services:
  api:
    image: copilotos-api:latest
    build: {}
  web:
    image: copilotos-web:latest
    build: {}
```

**Beneficio:** Previene que `docker compose up` reconstruya con target dev

### ‚úÖ **2. Documentaci√≥n de Configuraci√≥n de Vol√∫menes**
```yaml
# PRODUCCI√ìN - Usar vol√∫menes Docker nombrados
volumes:
  mongodb_data:
    name: copilotos-prod_mongodb_data
    driver: local

# NUNCA usar bind mounts para datos de producci√≥n
# NUNCA usar flag -v sin backup previo
```

---

## üö® PLAN DE ACCI√ìN URGENTE (SIGUIENTES 24 HORAS)

### **Priority 1: Sistema de Backups Automatizados** üî¥

#### **A. Backup Script de MongoDB**
```bash
#!/bin/bash
# /home/jf/scripts/backup-mongodb.sh

BACKUP_DIR="/home/jf/backups/mongodb"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=30

# Crear backup
docker exec copilotos-prod-mongodb mongodump \
  --uri="mongodb://copilotos_prod_user:ProdMongo2024!SecurePass@localhost:27017/copilotos?authSource=admin" \
  --gzip \
  --archive="/backup/copilotos_${DATE}.gz"

# Copiar del contenedor al host
docker cp copilotos-prod-mongodb:/backup/copilotos_${DATE}.gz ${BACKUP_DIR}/

# Limpiar backups antiguos
find ${BACKUP_DIR} -name "copilotos_*.gz" -mtime +${RETENTION_DAYS} -delete

# Log
echo "$(date): Backup completed - copilotos_${DATE}.gz" >> ${BACKUP_DIR}/backup.log
```

#### **B. Cron Job para Backups Autom√°ticos**
```cron
# Backup MongoDB cada 6 horas
0 */6 * * * /home/jf/scripts/backup-mongodb.sh

# Backup completo diario a las 2 AM
0 2 * * * /home/jf/scripts/backup-full-system.sh
```

#### **C. Backup de Vol√∫menes Docker**
```bash
#!/bin/bash
# /home/jf/scripts/backup-docker-volumes.sh

BACKUP_DIR="/home/jf/backups/docker-volumes"
DATE=$(date +%Y%m%d_%H%M%S)

# Backup MongoDB volume
docker run --rm \
  -v copilotos-prod_mongodb_data:/data \
  -v ${BACKUP_DIR}:/backup \
  alpine tar czf /backup/mongodb_${DATE}.tar.gz -C /data .

# Backup Redis volume (opcional pero recomendado)
docker run --rm \
  -v copilotos-prod_redis_data:/data \
  -v ${BACKUP_DIR}:/backup \
  alpine tar czf /backup/redis_${DATE}.tar.gz -C /data .
```

### **Priority 2: Procedimientos de Recuperaci√≥n Documentados** üü°

Crear `docs/DISASTER-RECOVERY.md` con:
1. Procedimiento completo de restore desde backup
2. Verificaci√≥n de integridad de backups
3. Procedimiento de rollback de deployment
4. Contactos de emergencia

### **Priority 3: Monitoreo de Backups** üü°

```bash
#!/bin/bash
# /home/jf/scripts/monitor-backups.sh

# Verificar que exista backup reciente (< 6 horas)
LATEST_BACKUP=$(find /home/jf/backups/mongodb -name "copilotos_*.gz" -mmin -360 | wc -l)

if [ $LATEST_BACKUP -eq 0 ]; then
  # Enviar alerta (configurar con servicio de alertas)
  echo "‚ö†Ô∏è  WARNING: No MongoDB backup found in last 6 hours!" | mail -s "BACKUP ALERT" admin@saptiva.com
fi
```

### **Priority 4: Configuraci√≥n de Vol√∫menes Consistente** üü¢

**Modificar `docker-compose.yml` base:**
```yaml
# NUNCA usar env_file hardcoded
# services:
#   mongodb:
#     env_file:  # <-- REMOVE THIS
#       - ../envs/.env
```

**Usar variables de entorno en su lugar:**
```yaml
services:
  mongodb:
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGODB_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGODB_PASSWORD}
```

---

## üìö LECCIONES APRENDIDAS

### **1. Backups No Son Opcionales**
> "Si no est√° en backup, no existe"

- Los backups deben ser autom√°ticos, frecuentes y verificados
- Pol√≠tica m√≠nima: Backup cada 6 horas + Daily + Weekly + Monthly
- Retenci√≥n: 30 d√≠as de backups horarios, 12 meses de backups mensuales

### **2. Flags Destructivos Requieren Confirmaci√≥n**
```bash
# MAL: Destructivo sin confirmaci√≥n
docker compose down -v

# MEJOR: Wrapper con confirmaci√≥n
function docker-compose-down-volumes() {
  echo "‚ö†Ô∏è  WARNING: This will DELETE all volumes!"
  echo "Volumes to be deleted:"
  docker compose config --volumes
  read -p "Are you sure? Type 'yes' to continue: " confirm
  if [ "$confirm" = "yes" ]; then
    docker compose down -v
  fi
}
```

### **3. Configuraci√≥n de Entornos Debe Ser Expl√≠cita**
- Evitar hardcoded env_file paths
- Usar COMPOSE_PROJECT_NAME consistente entre entornos
- Documentar claramente diferencias entre dev/staging/prod

### **4. Testing en Staging Es Obligatorio**
- NUNCA deployar a producci√≥n sin testing en staging
- Staging debe replicar producci√≥n lo m√°s cercano posible
- Incluir tests de disaster recovery en staging

### **5. Documentaci√≥n Es Cr√≠tica**
- Procedimientos de backup/restore deben estar documentados
- Runbooks para incidentes comunes
- Diagramas de arquitectura actualizados

---

## üìã CHECKLIST DE PREVENCI√ìN

### **Antes de Cada Deployment:**
- [ ] Backup manual de MongoDB tomado
- [ ] Verificar que backup autom√°tico m√°s reciente es < 6 horas
- [ ] Testing completo en staging/dev
- [ ] Verificar que docker-compose.override.yml existe en prod
- [ ] Verificar COMPOSE_PROJECT_NAME es consistente
- [ ] Plan de rollback documentado

### **Durante Deployment:**
- [ ] NO usar `docker compose down -v` sin backup reciente
- [ ] Verificar logs de MongoDB para errores de autenticaci√≥n
- [ ] Verificar que contenedores usan im√°genes correctas (production target)
- [ ] Verificar health checks de todos los servicios

### **Despu√©s de Deployment:**
- [ ] Verificar todos los servicios healthy
- [ ] Verificar datos de MongoDB intactos (sample queries)
- [ ] Verificar logs no tienen errores cr√≠ticos
- [ ] Tomar backup post-deployment
- [ ] Documentar cualquier issue encontrado

---

## üéØ M√âTRICAS DE √âXITO

Para considerar este incidente **resuelto y prevenido**, necesitamos:

### **M√©tricas T√©cnicas:**
- ‚úÖ Sistema de backups automatizados implementado y funcionando
- ‚úÖ Al menos 7 d√≠as de backups exitosos verificados
- ‚úÖ Procedimientos de disaster recovery documentados y testeados
- ‚úÖ docker-compose.override.yml permanente en producci√≥n
- ‚úÖ Alertas configuradas para fallos de backup

### **M√©tricas Operacionales:**
- ‚úÖ Zero data loss en pr√≥ximos 90 d√≠as
- ‚úÖ Tiempo de recuperaci√≥n de backup < 15 minutos (testeado)
- ‚úÖ 100% de deployments siguen checklist de prevenci√≥n

### **M√©tricas de Proceso:**
- ‚úÖ Staging environment configurado y en uso
- ‚úÖ Pre-deployment testing obligatorio implementado
- ‚úÖ Runbooks actualizados y accesibles

---

## üîó REFERENCIAS Y DOCUMENTOS RELACIONADOS

- `docs/operations/deployment.md` - Procedimientos de deployment
- `docs/DISASTER-RECOVERY.md` - Procedimientos de recuperaci√≥n (a crear)
- `scripts/backup-mongodb.sh` - Script de backup (a crear)
- `scripts/deploy-with-tar.sh` - Script de deployment (ya existe)

---

## ‚úçÔ∏è FIRMAS Y APROBACIONES

**Preparado por:** Claude (AI Assistant)
**Revisado por:** Jaziel Flores (jf@saptiva.com)
**Fecha:** 9 de Octubre, 2025
**Pr√≥xima Revisi√≥n:** 16 de Octubre, 2025 (verificar implementaci√≥n de medidas)

---

## üîÑ SEGUIMIENTO

| Acci√≥n | Responsable | Fecha L√≠mite | Estado |
|--------|-------------|--------------|---------|
| Implementar backup autom√°tico MongoDB | DevOps | 10-Oct-2025 | ‚è≥ Pendiente |
| Configurar cron jobs | DevOps | 10-Oct-2025 | ‚è≥ Pendiente |
| Crear DISASTER-RECOVERY.md | DevOps | 10-Oct-2025 | ‚è≥ Pendiente |
| Testear procedimiento de restore | DevOps | 11-Oct-2025 | ‚è≥ Pendiente |
| Configurar staging environment | DevOps | 13-Oct-2025 | ‚è≥ Pendiente |
| Implementar alertas de backup | DevOps | 11-Oct-2025 | ‚è≥ Pendiente |
| Review de este documento | Team Lead | 16-Oct-2025 | ‚è≥ Pendiente |

---

**ESTE INCIDENTE NO DEBE REPETIRSE.**

**Prioridad m√°xima: Implementar sistema de backups en las pr√≥ximas 24 horas.**
