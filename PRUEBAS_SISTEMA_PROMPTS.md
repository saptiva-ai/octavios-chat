# GuÃ­a de Pruebas: Sistema de Prompts por Modelo

## âœ… Sistema Actualizado y Funcionando

Los contenedores han sido actualizados con la implementaciÃ³n del **sistema de prompts por modelo**. El sistema estÃ¡ activo con `ENABLE_MODEL_SYSTEM_PROMPT=true`.

---

## ğŸ§ª Pruebas en la AplicaciÃ³n Web

### Prueba 1: Comparar Respuestas de Diferentes Modelos

Esta prueba demuestra cÃ³mo cada modelo tiene un comportamiento diferenciado segÃºn su configuraciÃ³n.

**Pasos:**
1. Abre la aplicaciÃ³n: `http://localhost:3000`
2. Inicia sesiÃ³n (o crea una cuenta si es necesario)
3. Crea una nueva conversaciÃ³n

**Mensaje de prueba:**
```
Dame 3 puntos clave sobre inteligencia artificial
```

**Prueba con cada modelo:**

#### a) **Saptiva Turbo** (Optimizado para velocidad)
- Selecciona: `Saptiva Turbo` en el selector de modelo
- EnvÃ­a el mensaje
- **Comportamiento esperado:**
  - Respuesta en â‰¤6 bullets (configurado con addendum)
  - Temperatura: 0.25 (mÃ¡s determinÃ­stica)
  - Respuesta concisa y rÃ¡pida
  - Max tokens: 1200 (canal "chat")

#### b) **Saptiva Cortex** (Optimizado para rigor)
- Cambia a: `Saptiva Cortex`
- EnvÃ­a el MISMO mensaje
- **Comportamiento esperado:**
  - Supuestos explÃ­citos
  - Nivel de confianza declarado
  - Temperatura: 0.35 (mÃ¡s variaciÃ³n)
  - Razonamiento mÃ¡s profundo

#### c) **Saptiva Ops** (Optimizado para cÃ³digo)
- Cambia a: `Saptiva Ops`
- Prueba con: `"Escribe una funciÃ³n Python que valide emails"`
- **Comportamiento esperado:**
  - CÃ³digo con pruebas incluidas
  - Temperatura: 0.2 (muy determinÃ­stica)
  - PrÃ¡cticas DevOps y seguridad
  - Snippets testeables

---

### Prueba 2: Validar Context Injection

Esta prueba verifica que el contexto se inyecta correctamente en el prompt del usuario.

**Pasos:**
1. Abre Chrome DevTools (F12)
2. Ve a la pestaÃ±a "Network"
3. Filtra por "chat"
4. EnvÃ­a un mensaje en la app
5. Inspecciona el payload del request a `/api/chat`

**QuÃ© verificar:**
```json
{
  "message": "Tu mensaje aquÃ­",
  "model": "Saptiva Turbo",
  "channel": "chat",  // â† DeberÃ­a aparecer
  "context": {        // â† DeberÃ­a contener datos de sesiÃ³n
    "chat_id": "...",
    "user_id": "..."
  }
}
```

**Comportamiento esperado:**
- El backend agrega automÃ¡ticamente `chat_id` y `user_id` al contexto
- El context se formatea en el mensaje del usuario

---

### Prueba 3: Verificar TelemetrÃ­a (Logs del Backend)

Esta prueba confirma que la telemetrÃ­a con hash SHA256 funciona correctamente.

**Pasos:**
1. EnvÃ­a un mensaje en la app
2. En la terminal, ejecuta:
```bash
docker logs copilotos-api 2>&1 | grep "system_hash\|request_id\|Built Saptiva payload"
```

**QuÃ© esperar en los logs:**
```json
{
  "event": "Built Saptiva payload with model-specific prompt",
  "model": "Saptiva Turbo",
  "channel": "chat",
  "request_id": "abc-123-def-456",
  "system_hash": "f962b2adf7b10997",  // Hash SHA256 (16 chars)
  "prompt_version": "v1",
  "max_tokens": 1200,
  "temperature": 0.25,
  "has_tools": false
}
```

**Validaciones:**
âœ… `system_hash` presente (NO el contenido del prompt)
âœ… `request_id` Ãºnico por request
âœ… `max_tokens` = 1200 para canal "chat"
âœ… `temperature` especÃ­fica del modelo

---

## ğŸ”¬ Pruebas con cURL (Sin UI)

Si prefieres probar directamente contra la API:

### Paso 1: Obtener Token de AutenticaciÃ³n

```bash
# Login
TOKEN=$(curl -X POST http://localhost:8001/api/auth/login \
  -H 'Content-Type: application/json' \
  -d '{
    "email": "tu-email@example.com",
    "password": "tu-password"
  }' | jq -r '.access_token')

echo "Token: $TOKEN"
```

### Paso 2: Probar Diferentes Modelos

#### a) **Saptiva Turbo** (Brevedad)
```bash
curl -X POST http://localhost:8001/api/chat \
  -H "Authorization: Bearer $TOKEN" \
  -H 'Content-Type: application/json' \
  -d '{
    "message": "Dame 3 bullets sobre IA",
    "model": "Saptiva Turbo",
    "channel": "chat",
    "context": {"test_id": "turbo-001"}
  }' | jq .
```

**Verificar en respuesta:**
- Respuesta concisa (â‰¤6 bullets)
- `model`: "Saptiva Turbo"

#### b) **Saptiva Cortex** (Rigor)
```bash
curl -X POST http://localhost:8001/api/chat \
  -H "Authorization: Bearer $TOKEN" \
  -H 'Content-Type: application/json' \
  -d '{
    "message": "Explica machine learning",
    "model": "Saptiva Cortex",
    "channel": "chat",
    "context": {"test_id": "cortex-001"}
  }' | jq .
```

**Verificar en respuesta:**
- Supuestos explÃ­citos
- Nivel de confianza mencionado

#### c) **Cambiar Canal** (max_tokens diferente)
```bash
# Canal "report" â†’ max_tokens: 3500
curl -X POST http://localhost:8001/api/chat \
  -H "Authorization: Bearer $TOKEN" \
  -H 'Content-Type: application/json' \
  -d '{
    "message": "Genera un reporte sobre tendencias de IA",
    "model": "Saptiva Cortex",
    "channel": "report",
    "context": {"test_id": "report-001"}
  }' | jq .
```

**Verificar en logs:**
```bash
docker logs copilotos-api 2>&1 | grep "max_tokens" | tail -1
```
DeberÃ­a mostrar: `"max_tokens": 3500`

---

## ğŸ¯ Pruebas Avanzadas

### Prueba 4: Feature Flag (Rollback)

Desactivar el sistema de prompts y volver a comportamiento legacy:

```bash
# 1. Editar envs/.env
nano envs/.env
# Cambiar: ENABLE_MODEL_SYSTEM_PROMPT=false

# 2. Reiniciar API
docker compose -f infra/docker-compose.yml --env-file envs/.env restart api

# 3. Verificar logs
docker logs copilotos-api 2>&1 | grep "legacy"
```

**Comportamiento esperado:**
- Logs muestran: `"legacy_mode": true`
- Prompts NO usan sistema nuevo
- Temperatura default: 0.7

**Revertir:**
```bash
# Cambiar de vuelta: ENABLE_MODEL_SYSTEM_PROMPT=true
docker compose -f infra/docker-compose.yml --env-file envs/.env restart api
```

---

### Prueba 5: Herramientas (Tools Injection)

Si tienes herramientas habilitadas:

```bash
curl -X POST http://localhost:8001/api/chat \
  -H "Authorization: Bearer $TOKEN" \
  -H 'Content-Type: application/json' \
  -d '{
    "message": "Busca informaciÃ³n actual sobre GPT-4",
    "model": "Saptiva Turbo",
    "channel": "chat",
    "tools_enabled": {
      "web_search": true
    }
  }' | jq .
```

**Verificar en logs:**
```bash
docker logs copilotos-api 2>&1 | grep "has_tools" | tail -1
```
DeberÃ­a mostrar: `"has_tools": true`

---

## ğŸ“Š Checklist de ValidaciÃ³n

Use esta checklist para confirmar que todo funciona:

### Funcionalidad BÃ¡sica
- [ ] âœ… API levanta sin errores (health check OK)
- [ ] âœ… Modelos diferentes dan respuestas diferenciadas
- [ ] âœ… Context injection funciona (chat_id, user_id)

### TelemetrÃ­a
- [ ] âœ… `system_hash` aparece en logs (16 chars)
- [ ] âœ… NO aparece contenido del prompt en logs
- [ ] âœ… `request_id` es Ãºnico por request
- [ ] âœ… `max_tokens` varÃ­a segÃºn canal (chat:1200, report:3500)
- [ ] âœ… `temperature` varÃ­a segÃºn modelo

### ConfiguraciÃ³n por Modelo
- [ ] âœ… **Saptiva Turbo**: Respuestas concisas (â‰¤6 bullets)
- [ ] âœ… **Saptiva Cortex**: Supuestos explÃ­citos
- [ ] âœ… **Saptiva Ops**: CÃ³digo con tests

### Feature Flag
- [ ] âœ… `ENABLE_MODEL_SYSTEM_PROMPT=false` â†’ legacy mode
- [ ] âœ… `ENABLE_MODEL_SYSTEM_PROMPT=true` â†’ nuevo sistema

---

## ğŸ› Troubleshooting

### Problema: "Token de autenticaciÃ³n requerido"
**SoluciÃ³n:** ObtÃ©n un token con `/api/auth/login` primero

### Problema: Logs no muestran `system_hash`
**SoluciÃ³n:**
1. Verifica `ENABLE_MODEL_SYSTEM_PROMPT=true` en `envs/.env`
2. Reinicia la API: `docker compose restart api`

### Problema: Modelos responden igual
**SoluciÃ³n:**
1. Verifica que `prompts/registry.yaml` tenga addendums diferentes
2. Compara valores de `temperature` en logs

### Problema: "Prompt registry not found"
**SoluciÃ³n:**
1. Verifica que existe: `ls apps/api/prompts/registry.yaml`
2. Verifica `PROMPT_REGISTRY_PATH` en `envs/.env`

---

## ğŸ“ˆ MÃ©tricas a Monitorear

En producciÃ³n, monitorea:

1. **Latencia por modelo:**
   - Turbo deberÃ­a ser mÃ¡s rÃ¡pido (temp: 0.25)
   - Cortex puede ser mÃ¡s lento (razonamiento profundo)

2. **DistribuciÃ³n de system_hash:**
   - DeberÃ­as ver ~5 hashes diferentes (uno por modelo)
   - Hash debe ser consistente para el mismo modelo

3. **Tool-call rate:**
   - Compara con/sin `tools_enabled`

4. **Error rate:**
   - DeberÃ­a ser similar a comportamiento legacy

---

## âœ… ValidaciÃ³n Final

Si completaste todas las pruebas y el checklist estÃ¡ OK:

**ğŸ‰ Sistema de Prompts por Modelo Funcionando Correctamente**

**PrÃ³ximos pasos:**
1. Crear PR para merge a `develop`
2. Desplegar en staging
3. Canary rollout: 10% â†’ 50% â†’ 100%

---

**Creado:** 2025-10-06
**Branch:** `feat/model-system-prompts`
**Commit:** `491d993`
